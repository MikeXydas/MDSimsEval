{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on 5HT2A Receptor\n",
    "\n",
    "Goal of this notebook is to use the features extracted and analyzed on the other Analysis notebook.  \n",
    "We want to differentiate agonist ligands from antagonists using features extracted from the MD simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T08:13:26.545789Z",
     "start_time": "2020-04-27T08:13:26.530959Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AnalysisActor.AnalysisActorClass import AnalysisActor\n",
    "from AnalysisActor.utils import create_analysis_actor_dict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "import re\n",
    "import os\n",
    "import subprocess\n",
    "import logging\n",
    "import math\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T09:00:19.399539Z",
     "start_time": "2020-04-25T09:00:19.396637Z"
    }
   },
   "source": [
    "## Reading the Simulations\n",
    "\n",
    "We will use the `AnalysisActor` package I wrote which is able to extract on low level features from the simulations. I call it low level since for example the `AnalysisActor.class` of a ligand will give us the $Rg$ of the ligand on each frame. This must be then reduced to features that I have analyzed like mean and std for each ligand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:47:51.352794Z",
     "start_time": "2020-04-27T07:45:37.890421Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agonists | Lorcaserin:   7%|▋         | 1/15 [00:02<00:41,  2.94s/it]/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/MDAnalysis/lib/mdamath.py:259: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  alpha = np.rad2deg(np.arccos(np.dot(y, z) / (ly * lz)))\n",
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/MDAnalysis/lib/mdamath.py:260: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  beta = np.rad2deg(np.arccos(np.dot(x, z) / (lx * lz)))\n",
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/MDAnalysis/lib/mdamath.py:261: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  gamma = np.rad2deg(np.arccos(np.dot(x, y) / (lx * ly)))\n",
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/MDAnalysis/lib/mdamath.py:264: RuntimeWarning: invalid value encountered in greater\n",
      "  if np.all(box > 0.0) and alpha < 180.0 and beta < 180.0 and gamma < 180.0:\n",
      "Agonists | Donitriptan: 100%|██████████| 15/15 [00:44<00:00,  2.94s/it]  \n",
      "Antagonists | Ziprasione: 100%|██████████| 18/18 [01:29<00:00,  4.96s/it]   \n"
     ]
    }
   ],
   "source": [
    "# Reading the simulations\n",
    "analysis_actors_dict = create_analysis_actor_dict('../datasets/New_AI_MD/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analysis_actors_dict` is a dictionary:\n",
    "```python \n",
    "{\n",
    "    \"Agonists\": List[AnalysisActor.class]\n",
    "    \"Antagonists\": List[AnalysisActor.class]\n",
    "}\n",
    "```\n",
    "This dictionary currently has only read the trajectories and has not calculated any of its metrics.  In order to do that we must call the `AnalysisActor.perform_analysis` method, which takes as an argument a list of metrics to be calculated.  \n",
    "  \n",
    "**Care**: The calculations need memory in order to be calculated and stored, so monitor the memory usage. If this becomes too big of a problem we can solve it in a \"dynamic\" way meaning that we will not keep saved the trajectories but demand them briefly for the calculations to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:49:05.654155Z",
     "start_time": "2020-04-27T07:47:51.370768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62317bd63bdc494dbd57a465aa33fcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Ligand Calculations', max=33.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Possible arguments for \"metrics\" list:\n",
    "#     Empty List [] (default): All of the available metrics will be calculated\n",
    "#     'Rg': Radius of Gyration\n",
    "#     'RMSF': Root Mean Square Fluctuations\n",
    "#     'SASA': Solvent Accessible Surface Area\n",
    "#     'PCA': Principal Component Analysis\n",
    "#     'Hbonds': Hydrogen Bonds\n",
    "#     'Salt': Calculate number of salt bridges\n",
    "\n",
    "# Iterate on all the ligands\n",
    "for which_ligand in tqdm(analysis_actors_dict['Agonists'] + analysis_actors_dict['Antagonists'], desc=\"Ligand Calculations\"):\n",
    "    which_ligand.perform_analysis(metrics=[\"Rg\", \"SASA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Features\n",
    "\n",
    "From the calculations we must now extract ML appropriate features. The features used in our case are:\n",
    "* Mean of Rg\n",
    "* Std of Rg\n",
    "* Mean of SASA\n",
    "* Std of SASA\n",
    "\n",
    "One parameter one must think of is the window of the features. Our simulations are of 2.500 and using all of them may  not be ideal. Our analysis actually shows that the event happens after 1.200 frames in most cases. However, as a starting point we will use all of the frames.  \n",
    "  \n",
    "**As labels we will use:**\n",
    "* Agonist: 1\n",
    "* Antagonist: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:22:22.674748Z",
     "start_time": "2020-04-27T10:22:22.647405Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(analysis_actors_dict, start=0, stop=2500):\n",
    "    \"\"\"\n",
    "    Creates the df containing the feature columns and as the last column the labels.\n",
    "    Currently the features are Rg mean, Rg std, SASA mean, SASA std.\n",
    "    \n",
    "    Args:\n",
    "        analysis_actors_dict:   {\n",
    "                                    \"Agonists\": List[AnalysisActor.class]\n",
    "                                    \"Antagonists\": List[AnalysisActor.class]\n",
    "                                }\n",
    "                                \n",
    "        start(int): The starting frame of the simulation we are using for the features\n",
    "        stop(int): The starting last of the simulation we are using for the features\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame of #ligands rows and #features + 1 (for the labels) columns \n",
    "        \n",
    "    \"\"\"\n",
    "    full_dataset = []\n",
    "    \n",
    "    # Iterate on the agonists\n",
    "    for which_ligand in analysis_actors_dict['Agonists']:\n",
    "        # Rg\n",
    "        mean_rg = np.mean(which_ligand.get_radius_of_gyration()[start:stop])\n",
    "        std_rg = np.std(which_ligand.get_radius_of_gyration()[start:stop])\n",
    "\n",
    "        # SASA\n",
    "        mean_sasa = np.mean(which_ligand.get_sasa()[1][start:stop])\n",
    "        std_sasa = np.std(which_ligand.get_sasa()[1][start:stop])\n",
    "\n",
    "        # For each ligand we will create a vector of [Mean Rg, Std Rg, Mean SASA, Std SASA, Ligand_Label]\n",
    "        full_dataset.append([mean_rg, std_rg, mean_sasa, std_sasa, 1])\n",
    "\n",
    "    # Iterate on the antagonists\n",
    "    for which_ligand in analysis_actors_dict['Antagonists']:\n",
    "        # Rg\n",
    "        mean_rg = np.mean(which_ligand.get_radius_of_gyration()[start:stop])\n",
    "        std_rg = np.std(which_ligand.get_radius_of_gyration()[start:stop])\n",
    "\n",
    "        # SASA\n",
    "        mean_sasa = np.mean(which_ligand.get_sasa()[1][start:stop])\n",
    "        std_sasa = np.std(which_ligand.get_sasa()[1][start:stop])\n",
    "\n",
    "        # For each ligand we will create a vector of [Mean Rg, Std Rg, Mean SASA, Std SASA, Ligand_Label]\n",
    "        full_dataset.append([mean_rg, std_rg, mean_sasa, std_sasa, 0])\n",
    "\n",
    "    dataset_df = pd.DataFrame(full_dataset, columns=['RgMean', 'RgStd', 'SASAMean', 'SASAstd', 'LigandLabel'])\n",
    "    \n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:23:14.691337Z",
     "start_time": "2020-04-27T10:23:14.656652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RgMean</th>\n",
       "      <th>RgStd</th>\n",
       "      <th>SASAMean</th>\n",
       "      <th>SASAstd</th>\n",
       "      <th>LigandLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.995239</td>\n",
       "      <td>0.090964</td>\n",
       "      <td>155.544030</td>\n",
       "      <td>2.831024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.086341</td>\n",
       "      <td>0.142383</td>\n",
       "      <td>159.022748</td>\n",
       "      <td>3.454086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.942304</td>\n",
       "      <td>0.098339</td>\n",
       "      <td>153.111026</td>\n",
       "      <td>4.523830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21.035371</td>\n",
       "      <td>0.121321</td>\n",
       "      <td>157.633546</td>\n",
       "      <td>3.535757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.076436</td>\n",
       "      <td>0.081224</td>\n",
       "      <td>160.071747</td>\n",
       "      <td>2.659344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.922859</td>\n",
       "      <td>0.090295</td>\n",
       "      <td>158.956388</td>\n",
       "      <td>2.429651</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.968421</td>\n",
       "      <td>0.072936</td>\n",
       "      <td>151.812842</td>\n",
       "      <td>2.365321</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.076800</td>\n",
       "      <td>0.090476</td>\n",
       "      <td>161.042717</td>\n",
       "      <td>2.753463</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20.884766</td>\n",
       "      <td>0.075554</td>\n",
       "      <td>154.199257</td>\n",
       "      <td>2.746676</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20.934949</td>\n",
       "      <td>0.081786</td>\n",
       "      <td>156.841953</td>\n",
       "      <td>2.723377</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20.989619</td>\n",
       "      <td>0.094354</td>\n",
       "      <td>156.219414</td>\n",
       "      <td>2.971510</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20.940189</td>\n",
       "      <td>0.080781</td>\n",
       "      <td>154.998263</td>\n",
       "      <td>2.728893</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21.103742</td>\n",
       "      <td>0.071256</td>\n",
       "      <td>157.194907</td>\n",
       "      <td>2.360701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20.988612</td>\n",
       "      <td>0.090061</td>\n",
       "      <td>151.452120</td>\n",
       "      <td>2.683555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20.925944</td>\n",
       "      <td>0.098747</td>\n",
       "      <td>155.469628</td>\n",
       "      <td>2.446606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20.942513</td>\n",
       "      <td>0.082318</td>\n",
       "      <td>158.135100</td>\n",
       "      <td>2.312263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20.978162</td>\n",
       "      <td>0.096010</td>\n",
       "      <td>153.654268</td>\n",
       "      <td>3.715580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20.960524</td>\n",
       "      <td>0.083844</td>\n",
       "      <td>155.899645</td>\n",
       "      <td>3.014519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20.971663</td>\n",
       "      <td>0.080782</td>\n",
       "      <td>153.075058</td>\n",
       "      <td>2.585485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.059064</td>\n",
       "      <td>0.128511</td>\n",
       "      <td>157.785909</td>\n",
       "      <td>4.197273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.004892</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>159.493212</td>\n",
       "      <td>2.511912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.168134</td>\n",
       "      <td>0.169499</td>\n",
       "      <td>163.136150</td>\n",
       "      <td>3.331997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.866374</td>\n",
       "      <td>0.072342</td>\n",
       "      <td>155.427480</td>\n",
       "      <td>2.825695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.069114</td>\n",
       "      <td>0.070248</td>\n",
       "      <td>157.437302</td>\n",
       "      <td>2.415028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20.929286</td>\n",
       "      <td>0.103610</td>\n",
       "      <td>153.583508</td>\n",
       "      <td>2.781337</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20.902666</td>\n",
       "      <td>0.070367</td>\n",
       "      <td>154.129323</td>\n",
       "      <td>2.895767</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.052017</td>\n",
       "      <td>0.087079</td>\n",
       "      <td>159.128248</td>\n",
       "      <td>2.436788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20.901733</td>\n",
       "      <td>0.073875</td>\n",
       "      <td>155.519390</td>\n",
       "      <td>2.441363</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20.869916</td>\n",
       "      <td>0.083383</td>\n",
       "      <td>153.134431</td>\n",
       "      <td>2.908245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20.898134</td>\n",
       "      <td>0.091606</td>\n",
       "      <td>155.416480</td>\n",
       "      <td>2.418714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>21.012846</td>\n",
       "      <td>0.066665</td>\n",
       "      <td>155.215628</td>\n",
       "      <td>2.511110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20.876435</td>\n",
       "      <td>0.071600</td>\n",
       "      <td>153.726783</td>\n",
       "      <td>2.726395</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20.984390</td>\n",
       "      <td>0.085265</td>\n",
       "      <td>158.830088</td>\n",
       "      <td>2.622056</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       RgMean     RgStd    SASAMean   SASAstd  LigandLabel\n",
       "0   20.995239  0.090964  155.544030  2.831024            1\n",
       "1   21.086341  0.142383  159.022748  3.454086            1\n",
       "2   20.942304  0.098339  153.111026  4.523830            1\n",
       "3   21.035371  0.121321  157.633546  3.535757            1\n",
       "4   21.076436  0.081224  160.071747  2.659344            1\n",
       "5   20.922859  0.090295  158.956388  2.429651            1\n",
       "6   20.968421  0.072936  151.812842  2.365321            1\n",
       "7   21.076800  0.090476  161.042717  2.753463            1\n",
       "8   20.884766  0.075554  154.199257  2.746676            1\n",
       "9   20.934949  0.081786  156.841953  2.723377            1\n",
       "10  20.989619  0.094354  156.219414  2.971510            1\n",
       "11  20.940189  0.080781  154.998263  2.728893            1\n",
       "12  21.103742  0.071256  157.194907  2.360701            1\n",
       "13  20.988612  0.090061  151.452120  2.683555            1\n",
       "14  20.925944  0.098747  155.469628  2.446606            1\n",
       "15  20.942513  0.082318  158.135100  2.312263            0\n",
       "16  20.978162  0.096010  153.654268  3.715580            0\n",
       "17  20.960524  0.083844  155.899645  3.014519            0\n",
       "18  20.971663  0.080782  153.075058  2.585485            0\n",
       "19  21.059064  0.128511  157.785909  4.197273            0\n",
       "20  21.004892  0.095600  159.493212  2.511912            0\n",
       "21  21.168134  0.169499  163.136150  3.331997            0\n",
       "22  20.866374  0.072342  155.427480  2.825695            0\n",
       "23  21.069114  0.070248  157.437302  2.415028            0\n",
       "24  20.929286  0.103610  153.583508  2.781337            0\n",
       "25  20.902666  0.070367  154.129323  2.895767            0\n",
       "26  21.052017  0.087079  159.128248  2.436788            0\n",
       "27  20.901733  0.073875  155.519390  2.441363            0\n",
       "28  20.869916  0.083383  153.134431  2.908245            0\n",
       "29  20.898134  0.091606  155.416480  2.418714            0\n",
       "30  21.012846  0.066665  155.215628  2.511110            0\n",
       "31  20.876435  0.071600  153.726783  2.726395            0\n",
       "32  20.984390  0.085265  158.830088  2.622056            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the features on the agonists\n",
    "start, stop = [0, 2500]\n",
    "display(create_dataset(analysis_actors_dict, start=0, stop=2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and Training\n",
    "\n",
    "\n",
    "The main problem in our task is the little number of data points. This means that a good (or bad) result may be random and not reflect the reality. Taking that into account we will apply known techniques in order to be as general as possible using k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris dataset is used for debugging\n",
    "iris = pd.read_csv(filepath_or_buffer='../datasets/misc/iris.csv')\n",
    "\n",
    "X = np.array(iris)[:, :-1]\n",
    "y = np.array(iris)[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T11:00:26.440799Z",
     "start_time": "2020-04-27T11:00:26.421053Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_params(estimator, X, y, folds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Given an object estimator calculate Acc, f1, rec, auc using k-fold cross validation. This can be further\n",
    "    expanded to create confusion matrices.\n",
    "    \n",
    "    Args:\n",
    "        estimator (estimator Object): Object implementing the scikit-learn estimator interface\n",
    "        X (np.array): Matrix of the features of the input dataset\n",
    "        y (np.array): Vector of the labels of the ligands\n",
    "        folds (int): Number of fold of the CV\n",
    "        verbose (boolean): Currently chooses if we will print the per fold metrics or the total average metrics only\n",
    "    \"\"\"\n",
    "    \n",
    "    # This dict will be used to save the metrics of each fold\n",
    "    total_metrics_train = {\n",
    "        \"acc\": 0,\n",
    "        \"f1\": 0,\n",
    "        \"rec\": 0,\n",
    "        \"auc\": 0\n",
    "    }\n",
    "\n",
    "    total_metrics_test = {\n",
    "        \"acc\": 0,\n",
    "        \"f1\": 0,\n",
    "        \"rec\": 0,\n",
    "        \"auc\": 0\n",
    "    }\n",
    "\n",
    "    def calculate_metrics(y_true, y_pred, total_metrics, y_pred_probs=None, print_enabled=True):\n",
    "        # Calculate metrics\n",
    "        acc = metrics.accuracy_score(y_true, y_pred)\n",
    "        f1 = metrics.f1_score(y_true, y_pred)\n",
    "        rec = metrics.recall_score(y_true, y_pred)\n",
    "        if y_pred_probs is not None:\n",
    "            auc = metrics.roc_auc_score(y_true, y_pred_probs)\n",
    "\n",
    "        # Update total metrics\n",
    "        total_metrics['acc'] += acc\n",
    "        total_metrics['f1'] += f1\n",
    "        total_metrics['rec'] += rec\n",
    "        if y_pred_probs is not None:\n",
    "            total_metrics['auc'] += auc\n",
    "\n",
    "        # Print metrics\n",
    "        if print_enabled:\n",
    "            print(f'\\t\\tAccuraccy: {acc}')\n",
    "            print(f'\\t\\tRecall: {rec}')\n",
    "            print(f'\\t\\tF1_Score: {f1}')\n",
    "            if y_pred_probs is not None:\n",
    "                print(f'\\t\\tAUC: {auc}')\n",
    "\n",
    "    def print_total_metrics(total_metrics, splits):\n",
    "        print(f'> Averaged Metrics on the {splits}-folds')\n",
    "        print(f'\\tAccuraccy: {total_metrics[\"acc\"] / splits}')\n",
    "        print(f'\\tRecall: {total_metrics[\"rec\"] / splits}')\n",
    "        print(f'\\tF1_Score: {total_metrics[\"f1\"] / splits}')\n",
    "        if total_metrics['auc'] != 0:\n",
    "            print(f'\\tAUC: {total_metrics[\"auc\"] / splits}')\n",
    "\n",
    "    which_split = 0\n",
    "    kf = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        if verbose:\n",
    "            print(f'> Split: {which_split}')\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index].astype(int), y[test_index].astype(int)\n",
    "\n",
    "        # We will start with simple models like Logistic Regression\n",
    "        clf = estimator\n",
    "    \n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        # Predict on training set\n",
    "        train_pred = clf.predict(X_train)\n",
    "        train_pred_proba = clf.predict_proba(X_train)[:, 1]\n",
    "\n",
    "        # Predict on test set\n",
    "        test_pred = clf.predict(X_test)\n",
    "        test_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        # Metrics on the train set\n",
    "        if verbose:\n",
    "            print('\\tTraining Metrics')\n",
    "        calculate_metrics(y_train, train_pred, total_metrics_train, y_pred_probs=train_pred_proba, print_enabled=verbose)\n",
    "\n",
    "        # Metrics on the validation set\n",
    "        if verbose:\n",
    "            print('\\tValidation Metrics')\n",
    "        calculate_metrics(y_test, test_pred, total_metrics_test, y_pred_probs=test_pred_proba, print_enabled=verbose)\n",
    "\n",
    "        which_split += 1\n",
    "\n",
    "    print_total_metrics(total_metrics_train, which_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning\n",
    "\n",
    "Although, above we have working code of fitting and evaluating a simple Logistic Regression model we must now focus on finding the best parameters for the model. Since our dataset is really small we can easily run GridSearch on all the the hyper parameters of LR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T10:54:41.329742Z",
     "start_time": "2020-04-27T10:54:41.296866Z"
    }
   },
   "outputs": [],
   "source": [
    "def tuning(estimator, parameters, scores, X, y, verbose=True):\n",
    "    \"\"\"\n",
    "    Will run a GridSearch on the parameters of the given estimator and print the results and the best parameters\n",
    "    for each score we have given.\n",
    "    Care that the printed results are not sorted.\n",
    "    \n",
    "    Args:\n",
    "        estimator (estimator class): Class name implementing the scikit-learn estimator interface\n",
    "        parameters (dict): Dictionary of available parameters for the estimator\n",
    "        scores List[(str): List of string of metrics we want to maximize, ref:https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "        X (np.array): Our development training input\n",
    "        y (np.array): A vector of our development labels\n",
    "        verbose(boolean): Sets if we will be printing the results (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of metric names as keys and the best parameters as values\n",
    "    \"\"\"\n",
    "    best_params_list = []\n",
    "    for score in scores:\n",
    "        if verbose:\n",
    "            print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "            print()\n",
    "\n",
    "        clf = GridSearchCV(\n",
    "            estimator(), parameters, scoring='%s' % score\n",
    "        )\n",
    "        clf.fit(X, y)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Best parameters set found on development set:\")\n",
    "            print()\n",
    "            print(clf.best_params_)\n",
    "        \n",
    "        best_params_list.append(clf.best_params_)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print(\"Grid scores on development set:\")\n",
    "            print()\n",
    "            means = clf.cv_results_['mean_test_score']\n",
    "            stds = clf.cv_results_['std_test_score']\n",
    "            for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                      % (mean, std * 2, params))\n",
    "            print('\\n')\n",
    "    \n",
    "    # Create the returned dictionary of \"metric\": best_params\n",
    "    return dict(zip(scores, best_params_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "Having created modular methods for:\n",
    "* Creating the dataset\n",
    "* Tuning the hyperparameters\n",
    "* Evaluating the tuned parameters\n",
    "\n",
    "we will now combine them in one cell.  \n",
    "  \n",
    "**Disclaimer**: I have not included in the below part the creation of the `analysis_actors_dict` or the performing of the calculations on them, but they can be easily added on \"production\" code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T11:02:03.934340Z",
     "start_time": "2020-04-27T11:01:48.793026Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikexydas/pythonEnvs/thesisEnv/lib/python3.6/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Metric Maximized: accuracy\n",
      "> Averaged Metrics on the 5-folds\n",
      "\tAccuraccy: 0.5452991452991454\n",
      "\tRecall: 0.0\n",
      "\tF1_Score: 0.0\n",
      "\tAUC: 0.5\n",
      "\n",
      "\n",
      ">>> Metric Maximized: f1\n",
      "> Averaged Metrics on the 5-folds\n",
      "\tAccuraccy: 0.5606837606837607\n",
      "\tRecall: 0.13333333333333336\n",
      "\tF1_Score: 0.193015873015873\n",
      "\tAUC: 0.5562698412698414\n",
      "\n",
      "\n",
      ">>> Metric Maximized: recall\n",
      "> Averaged Metrics on the 5-folds\n",
      "\tAccuraccy: 0.5672364672364673\n",
      "\tRecall: 0.13333333333333336\n",
      "\tF1_Score: 0.18968253968253967\n",
      "\tAUC: 0.5917460317460318\n",
      "\n",
      "\n",
      ">>> Metric Maximized: roc_auc\n",
      "> Averaged Metrics on the 5-folds\n",
      "\tAccuraccy: 0.5452991452991454\n",
      "\tRecall: 0.0\n",
      "\tF1_Score: 0.0\n",
      "\tAUC: 0.5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We need the Reading part to be executed before this cell so as we have the complete analysis_actors_dict\n",
    "\n",
    "dataset_df = create_dataset(analysis_actors_dict, start=0, stop=2500)\n",
    "\n",
    "# Separate training data from labels, the rows are fully separated meaning that \n",
    "# first k rows are agonists and then we have the antagonists\n",
    "X = np.array(dataset_df)[:, :-1]\n",
    "y = np.array(dataset_df)[:, -1]\n",
    "\n",
    "# Select an estimator, CARE: We need the class and not an object eg LogistingRegression -> correct\n",
    "#                                                                   LogisticRegression() -> wrong\n",
    "estim = LogisticRegression\n",
    "\n",
    "# Select on which parameters we will perform Grid Search\n",
    "parameters = {\"C\":np.logspace(-5,1,7), \"penalty\":[\"l1\",\"l2\"], \"tol\": np.logspace(-9, -2, 8),\n",
    "             \"solver\":[\"liblinear\"], \"max_iter\": [100000]}\n",
    "\n",
    "# Select which metrics we are trying to maximize\n",
    "scores = [\"accuracy\", \"f1\", \"recall\", \"roc_auc\"]\n",
    "\n",
    "# This dictionary contains the best parameteres for each metric\n",
    "best_params = tuning(estim, parameters, scores, X, y, verbose=False)\n",
    "\n",
    "# Final evaluation on the best params for each metric\n",
    "for score in scores:\n",
    "    print(f'>>> Metric Maximized: {score}')\n",
    "    evaluate_params(estim(**best_params[score]), X, y, folds=5, verbose=False)\n",
    "    print('\\n') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('thesisEnv': virtualenv)",
   "language": "python",
   "name": "python36964bitthesisenvvirtualenv849bc23effdd4f5cbcfcfcad50606969"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
